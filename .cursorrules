# Agentic Morning Digest - Cursor Rules

## Project Identity
You are working on the **Agentic Morning Digest** - a one-day MVP that generates personalized daily digests mixing "need-to-know" (AI/news/history/politics) with "nice-to-know" (quotes, trivia, what-ifs). The output is a Streamlit app with a visible "Agent Thinking Log."

## Core Architecture & Data Flow
```
User Prefs ‚Üí Agent System (Real/Mock) ‚Üí Structured Output ‚Üí Presenter.render()
                    ‚Üì
              Agent Log (tracks all decisions)
```

### Module Responsibilities
- **app.py**: Streamlit UI + wiring + Agent Log display
- **agent/core.py**: Real agent (live data) + Mock agent (fallback) + structured output
- **agent/tools/**: LangChain tools for data retrieval and content processing
- **presenter.py**: Pure rendering helpers (no business logic)
- **prefs.py**: Load/save user preferences (JSON / session_state)
- **data/static_samples.json**: Fallback content for demo resilience

## Tech Stack
- **Frontend**: Streamlit
- **LLM**: GPT-4o (temperature=0.2, deterministic)
- **Agent Framework**: LangGraph + LangChain for AI agent orchestration
- **Structured Output**: Pydantic schemas with tool calling
- **Retrieval**: requests + BeautifulSoup for web scraping
- **Storage**: In-memory / JSON file for prefs + cached content
- **Audio**: OpenAI TTS API for AI voiceover (‚úÖ IMPLEMENTED)
- **Python**: 3.11+ with type hints

## Section Contract
Each section is a dict:
```python
{
  "id": "quick_hits" | "deep_dive" | "did_you_know" | "fun_spark" | "quote",
  "title": "üìå OpenAI Releases GPT-5 with 10x Performance Improvements",
  "kind": "need" | "nice",
  "items": [{"text": "The new model shows significant advances in reasoning, coding, and multimodal capabilities while reducing compute costs.", "url": optional}]
}
```

## Coding Standards
- **Python 3.11+** with type hints
- **Small pure functions** with docstrings
- **Black formatting** standard
- **No secrets in code** - use environment variables (OPENAI_API_KEY)
- **@st.cache_data(ttl=600)** for network calls
- **Timebox scraping** to 3s/site, then fallback
- **LLM calls**: temperature=0.2, bounded output with explicit limits
- **Structured Output**: Use Pydantic schemas with tool calling, avoid custom parsing

## Agent Architecture
- **Real Agent**: Uses live data sources (Hacker News + Tavily Search) with structured Pydantic output
- **Mock Agent**: Fallback agent using static/cached data for reliability
- **Structured Output**: All agents return `DigestResponse` schema with exactly 10 sections
- **Tool Integration**: LangChain tools for data retrieval and content processing
- **Fallback Chain**: Real Agent ‚Üí Mock Agent ‚Üí Static Samples
- **Current Live Tools**: `scrape_hacker_news`, `tavily_search` (more tools planned for future releases)

## Fallback Strategy
Always implement: **Live Data ‚Üí Static Samples**
- Log failures and switch reasons
- Keep UI responsive, never crash app
- Regenerate only failed sections, not whole page

## Agentic Workflow Rules
1. **Planning**: If topics include AI/History/Politics ‚Üí select `quick_hits`, `deep_dive` (need)
2. **Always include** one `nice` block (`quote` or `fun_spark`)
3. **Interleave pattern**: need ‚Üí nice ‚Üí need (10 sections)
4. **Item Descriptions**: Use items' text as descriptions; for Hacker News items without descriptions, generate sensible AI placeholder descriptions
5. **Log all decisions** to "Agent Thinking Log"
6. **Observability**: Every step appends concise log: `[Planner] chose X`, `[Retriever] cache hit for HN`

## AI Voiceover System
### Architecture
- **Script Generation**: GPT-4o converts digest sections into engaging voiceover scripts
- **Text-to-Speech**: OpenAI TTS API with 6 voice options (Alloy, Echo, Fable, Onyx, Nova, Shimmer)
- **Audio Storage**: Files saved to `app/voiceover/generated/` with unique UUID names
- **Audio Player**: Streamlit native audio player with file path serving

### User Flow
1. Generate digest ‚Üí Click "üéôÔ∏è Generate Voiceover" ‚Üí AI creates script ‚Üí TTS generates audio ‚Üí Play in browser
2. Voice selection in sidebar preferences
3. Script viewable in expandable section
4. Audio files persist in generated/ directory

## Data Sources
### Currently Implemented (Live Data)
- **Hacker News** (front page) ‚Üí "Quick Hits" tech/AI headlines (‚úÖ LIVE)
  - Note: Hacker News items don't have descriptions, so AI generates sensible placeholder descriptions
- **Tavily Search** ‚Üí Creative, random fun facts and niche content (‚úÖ LIVE)
  - Uses multiple diverse, creative search queries for surprising content
  - Avoids generic topics in favor of obscure, interesting, and weird facts
  - Searches for 2-3 completely different random topics per digest

### Planned for Future Implementation
- **Wikipedia ‚Äì Selected anniversaries** ‚Üí "Did You Know" history bits (üìã PLANNED)
- **quotes.toscrape.com** ‚Üí "Quote / Fun Spark" (üìã PLANNED)
- **Additional news sources** for broader coverage (üìã PLANNED)

## Implementation Status
### ‚úÖ Completed
1. **MVP Foundation**: app.py skeleton with sidebar prefs, Generate button, Agent Log expander
2. **Agent System**: Real agent with live data + Mock agent fallback
3. **Structured Output**: Pydantic schemas with tool calling for consistent parsing
4. **Tool Integration**: LangChain tools for data retrieval (Hacker News + Tavily Search implemented)
5. **Presentation Layer**: Build presenter.render() with markdown cards and collapsible expanders
6. **Static Data**: Create data/static_samples.json with sample content for fallback
7. **10-Section Generation**: Real agent now generates exactly 10 sections with need/nice balance
8. **User Agent Toggle**: UI control to switch between real and mock agents
9. **AI Voiceover System**: Complete TTS integration with script generation and audio playback
10. **Title/Description Structure**: Proper section titles as headlines with item text as descriptions
11. **Audio Player Integration**: Working audio playback with file storage in voiceover/generated/

### üìã Next Steps
1. **Additional Live Data Sources**: Wikipedia anniversaries, quotes.toscrape.com
2. **Enhanced Content Processing**: More sophisticated content analysis and categorization
3. **User Preferences**: More granular control over content types and sources
4. **Voice Customization**: More voice options and speech rate controls

## Planned Features
### üéØ High Priority
- **Additional Data Sources**: Expand beyond Hacker News and Tavily Search to include Wikipedia, quotes, and other news sources

### üîÑ Medium Priority  
- **Enhanced Processing**: More sophisticated content analysis and categorization algorithms
- **Advanced Preferences**: Granular user control over content types, sources, and presentation
- **Voice Customization**: More voice options, speech rate controls, and audio quality settings

## Demo Constraints
- Must show: pick preferences ‚Üí show "Agent plan" ‚Üí 10 interleaved sections (need/nice) ‚Üí regenerate
- Fast and reliable: prefer **cached/mocked** data if scraping/LLM is slow
- 3-minute screen recording capability
- Currently demonstrates: Hacker News + Tavily Search live data ‚Üí 10-section digest with agent thinking log ‚Üí AI voiceover
- Demo flow: Generate digest ‚Üí Generate voiceover ‚Üí Listen to AI-hosted morning digest
- Audio files stored in: app/voiceover/generated/ directory

## Non-Goals
- Building auth, multi-user infrastructure
- Production database setup
- Fancy styling beyond functional UI
- Real-time crawling beyond the three sources

## LangGraph Resources
Reference documentation in `/tutorials/langgraph-docs/`:
- Agent Overview: `/agents/overview.md`
- Prebuilt Components: `/agents/prebuilt.md`
- Multi-Agent Systems: `/agents/multi-agent.md`
- Memory Integration and Human-in-the-loop patterns

## Development Commands
- **Environment**: Use `news_push` conda/virtual environment
- **Install**: `pip install -r requirements.txt`
- **Run**: `cd app && streamlit run app.py`
- **Environment**: Ensure `OPENAI_API_KEY` is set

## Code Quality Rules
- Prefer elegant, direct solutions over complicated approaches
- Avoid code smells and suggest refactoring when beneficial
- Verify assumptions by examining existing code before making changes
- Use semantic search to understand codebase before implementing features
- Maintain clean separation of concerns between modules
- Always implement proper error handling and logging
