# Agentic Morning Digest - Cursor Rules

## Project Identity
You are working on the **Agentic Morning Digest** - a one-day MVP that generates personalized daily digests mixing "need-to-know" (AI/news/history/politics) with "nice-to-know" (quotes, trivia, what-ifs). The output is a Streamlit app with a visible "Agent Thinking Log."

## Core Architecture & Data Flow
```
User Prefs â†’ Agent System (Real/Mock) â†’ Structured Output â†’ Presenter.render()
                    â†“
              Agent Log (tracks all decisions)
```

### Module Responsibilities
- **app.py**: Streamlit UI + wiring + Agent Log display
- **agent/core.py**: Real agent (live data) + Mock agent (fallback) + structured output
- **agent/tools/**: LangChain tools for data retrieval and content processing
- **presenter.py**: Pure rendering helpers (no business logic)
- **prefs.py**: Load/save user preferences (JSON / session_state)
- **data/static_samples.json**: Fallback content for demo resilience

## Tech Stack
- **Frontend**: Streamlit
- **LLM**: GPT-4o (temperature=0.2, deterministic)
- **Agent Framework**: LangGraph + LangChain for AI agent orchestration
- **Structured Output**: Pydantic schemas with tool calling
- **Retrieval**: requests + BeautifulSoup for web scraping
- **Storage**: In-memory / JSON file for prefs + cached content
- **Audio**: TTS (Text-to-Speech) for AI voiceover (planned)
- **Python**: 3.11+ with type hints

## Section Contract
Each section is a dict:
```python
{
  "id": "quick_hits" | "deep_dive" | "did_you_know" | "fun_spark" | "quote",
  "title": "ðŸ“Œ OpenAI Releases GPT-5 with 10x Performance Improvements",
  "kind": "need" | "nice",
  "items": [{"text": "The new model shows significant advances in reasoning, coding, and multimodal capabilities while reducing compute costs.", "url": optional}]
}
```

## Coding Standards
- **Python 3.11+** with type hints
- **Small pure functions** with docstrings
- **Black formatting** standard
- **No secrets in code** - use environment variables (OPENAI_API_KEY)
- **@st.cache_data(ttl=600)** for network calls
- **Timebox scraping** to 3s/site, then fallback
- **LLM calls**: temperature=0.2, bounded output with explicit limits
- **Structured Output**: Use Pydantic schemas with tool calling, avoid custom parsing

## Agent Architecture
- **Real Agent**: Uses live data sources (currently Hacker News only) with structured Pydantic output
- **Mock Agent**: Fallback agent using static/cached data for reliability
- **Structured Output**: All agents return `DigestResponse` schema with exactly 10 sections
- **Tool Integration**: LangChain tools for data retrieval and content processing
- **Fallback Chain**: Real Agent â†’ Mock Agent â†’ Static Samples
- **Current Live Tools**: `scrape_hacker_news` (more tools planned for future releases)

## Fallback Strategy
Always implement: **Live Data â†’ Static Samples**
- Log failures and switch reasons
- Keep UI responsive, never crash app
- Regenerate only failed sections, not whole page

## Agentic Workflow Rules
1. **Planning**: If topics include AI/History/Politics â†’ select `quick_hits`, `deep_dive` (need)
2. **Always include** one `nice` block (`quote` or `fun_spark`)
3. **Interleave pattern**: need â†’ nice â†’ need (10 sections)
4. **Item Descriptions**: Use items' text as descriptions; for Hacker News items without descriptions, generate sensible AI placeholder descriptions
5. **Log all decisions** to "Agent Thinking Log"
6. **Observability**: Every step appends concise log: `[Planner] chose X`, `[Retriever] cache hit for HN`

## Data Sources
### Currently Implemented (Live Data)
- **Hacker News** (front page) â†’ "Quick Hits" tech/AI headlines (âœ… LIVE)
  - Note: Hacker News items don't have descriptions, so AI generates sensible placeholder descriptions

### Planned for Future Implementation
- **Wikipedia â€“ Selected anniversaries** â†’ "Did You Know" history bits (ðŸ“‹ PLANNED)
- **quotes.toscrape.com** â†’ "Quote / Fun Spark" (ðŸ“‹ PLANNED)
- **Additional news sources** for broader coverage (ðŸ“‹ PLANNED)

## Implementation Status
### âœ… Completed
1. **MVP Foundation**: app.py skeleton with sidebar prefs, Generate button, Agent Log expander
2. **Agent System**: Real agent with live data + Mock agent fallback
3. **Structured Output**: Pydantic schemas with tool calling for consistent parsing
4. **Tool Integration**: LangChain tools for data retrieval (Hacker News implemented)
5. **Presentation Layer**: Build presenter.render() with markdown cards and collapsible expanders
6. **Static Data**: Create data/static_samples.json with sample content for fallback
7. **10-Section Generation**: Real agent now generates exactly 10 sections with need/nice balance
8. **User Agent Toggle**: UI control to switch between real and mock agents

### ðŸ“‹ Next Steps
1. **AI Voiceover**: Add AI voiceover to the digest, speaking like a friendly morning digest host
2. **Additional Live Data Sources**: Wikipedia anniversaries, quotes.toscrape.com
3. **Enhanced Content Processing**: More sophisticated content analysis and categorization
4. **User Preferences**: More granular control over content types and sources

## Planned Features
### ðŸŽ¯ High Priority
- **AI Voiceover**: TTS integration with friendly morning digest host persona for audio experience

### ðŸ”„ Medium Priority  
- **Additional Data Sources**: Expand beyond Hacker News to include Wikipedia, quotes, and other news sources
- **Enhanced Processing**: More sophisticated content analysis and categorization algorithms
- **Advanced Preferences**: Granular user control over content types, sources, and presentation

## Demo Constraints
- Must show: pick preferences â†’ show "Agent plan" â†’ 10 interleaved sections (need/nice) â†’ regenerate
- Fast and reliable: prefer **cached/mocked** data if scraping/LLM is slow
- 3-minute screen recording capability
- Currently demonstrates: Hacker News live data â†’ 10-section digest with agent thinking log
- Future demo: Include AI voiceover as friendly morning digest host

## Non-Goals
- Building auth, multi-user infrastructure
- Production database setup
- Fancy styling beyond functional UI
- Real-time crawling beyond the three sources

## LangGraph Resources
Reference documentation in `/tutorials/langgraph-docs/`:
- Agent Overview: `/agents/overview.md`
- Prebuilt Components: `/agents/prebuilt.md`
- Multi-Agent Systems: `/agents/multi-agent.md`
- Memory Integration and Human-in-the-loop patterns

## Development Commands
- **Environment**: Use `news_push` conda/virtual environment
- **Install**: `pip install -r requirements.txt`
- **Run**: `cd app && streamlit run app.py`
- **Environment**: Ensure `OPENAI_API_KEY` is set

## Code Quality Rules
- Prefer elegant, direct solutions over complicated approaches
- Avoid code smells and suggest refactoring when beneficial
- Verify assumptions by examining existing code before making changes
- Use semantic search to understand codebase before implementing features
- Maintain clean separation of concerns between modules
- Always implement proper error handling and logging
